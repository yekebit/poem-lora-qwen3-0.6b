import torch
import torch.nn as nn
import torch.nn.functional as F


# ================================
# 1. 标准 FFN（前馈网络）—— 这就是“专家”的真实形态
# ================================
class FeedForward(nn.Module):
    """
    一个标准的 Transformer 前馈网络（FFN）：
      x → Linear → GELU/SiLU → Dropout → Linear
    在 Llama/Qwen 中，中间层维度通常是输入的 4 倍（如 4096 → 11008）
    """
    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim)      # 第一个线性层（升维）
        self.w2 = nn.Linear(hidden_dim, dim)      # 第二个线性层（降维）
        self.dropout = nn.Dropout(dropout)
        # 注意：Llama 用 SiLU，GPT 用 GELU，这里用 GELU（更通用）
        self.act = nn.GELU()

    def forward(self, x):
        # x: [batch, seq_len, dim]
        x = self.w1(x)          # [B, L, hidden_dim]
        x = self.act(x)         # 激活函数
        x = self.dropout(x)
        x = self.w2(x)          # [B, L, dim]
        return x


# ================================
# 2. 稀疏 MoE 层 —— 用多个 FFN 作为专家
# ================================
class SparseMoELayer(nn.Module):
    """
    稀疏 Mixture of Experts 层：
      - 门控网络选择 top-k 专家
      - 每个专家是一个完整的 FFN
      - 只有被选中的专家参与计算（梯度稀疏）
    """
    def __init__(
        self,
        num_experts: int,
        top_k: int,
        dim: int,               # 输入/输出维度（如 4096）
        hidden_dim: int,        # FFN 中间层维度（如 11008）
        dropout: float = 0.1
    ):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.dim = dim

        # 创建 num_experts 个独立的 FFN 专家
        self.experts = nn.ModuleList([
            FeedForward(dim, hidden_dim, dropout) for _ in range(num_experts)
        ])

        # 门控网络：输入 → 专家 logits
        # 注意：这里只用一个 Linear，不加激活函数
        self.gate = nn.Linear(dim, num_experts, bias=False)

    def forward(self, x):
        """
        输入: x.shape = [batch_size, seq_len, dim]
        输出: y.shape = [batch_size, seq_len, dim]
        """
        batch_size, seq_len, dim = x.shape
        assert dim == self.dim, f"输入维度 {dim} 不匹配 MoE 层维度 {self.dim}"

        # Step 1: 展平 token 维度，便于处理
        # [B, L, D] → [B*L, D]
        x_flat = x.view(-1, dim)  # total_tokens = B * L

        # Step 2: 门控打分
        gate_logits = self.gate(x_flat)  # [B*L, E]

        # Step 3: 选择 top-k 专家
        # topk_vals: top-k 的 logits 值       → [B*L, k]
        # topk_idxs: top-k 的专家索引        → [B*L, k]
        topk_vals, topk_idxs = torch.topk(gate_logits, self.top_k, dim=-1)
        topk_weights = F.softmax(topk_vals, dim=-1)  # [B*L, k]，归一化

        # Step 4: 初始化输出（全零）
        output_flat = torch.zeros_like(x_flat)  # [B*L, D]

        # Step 5: 【核心】遍历每个专家，累加其贡献
        for i, expert in enumerate(self.experts):
            # 找出哪些 token 选择了当前专家 i
            selected = (topk_idxs == i)  # [B*L, k], bool

            if not selected.any():
                continue  # 如果没人选这个专家，跳过

            # 提取每个 token 分配给专家 i 的总权重
            # 将未选中的位置置 0，然后对 k 个位置求和
            weights = torch.where(selected, topk_weights, 0.0).sum(dim=-1)  # [B*L]

            # 计算当前专家对所有 token 的输出
            expert_out = expert(x_flat)  # [B*L, D]

            # 加权累加：weights [B*L, 1] * expert_out [B*L, D] → [B*L, D]
            output_flat += weights.unsqueeze(-1) * expert_out

        # Step 6: 恢复原始形状
        output = output_flat.view(batch_size, seq_len, dim)  # [B, L, D]
        return output


# ================================
# 3. 完整的 MoE Transformer 层
# ================================
class MoETransformerLayer(nn.Module):
    """
    一个完整的 Transformer 层，其中 FFN 被替换为 MoE。
    结构：
      x → LayerNorm → Self-Attention → Residual → LayerNorm → MoE → Residual
    """
    def __init__(
        self,
        dim: int,
        num_heads: int,
        num_experts: int,
        top_k: int,
        hidden_dim: int,
        dropout: float = 0.1
    ):
        super().__init__()
        self.dim = dim

        # ========== Self-Attention 部分（标准实现）==========
        self.attn_norm = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(
            embed_dim=dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True  # 输入 shape: [B, L, D]
        )

        # ========== MoE FFN 部分 ==========
        self.moe_norm = nn.LayerNorm(dim)
        self.moe = SparseMoELayer(
            num_experts=num_experts,
            top_k=top_k,
            dim=dim,
            hidden_dim=hidden_dim,
            dropout=dropout
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, attn_mask=None):
        """
        x: [batch_size, seq_len, dim]
        attn_mask: 可选的 attention mask
        """
        # --- Self-Attention ---
        residual = x
        x = self.attn_norm(x)
        attn_output, _ = self.attn(x, x, x, attn_mask=attn_mask)
        x = residual + self.dropout(attn_output)

        # --- MoE FFN ---
        residual = x
        x = self.moe_norm(x)
        moe_output = self.moe(x)
        x = residual + self.dropout(moe_output)

        return x


# ================================
# 4. 使用示例
# ================================
if __name__ == "__main__":
    # 超参数（模拟 Llama 风格）
    dim = 256          # 模型维度（小规模演示）
    hidden_dim = 512   # FFN 中间层（通常 2~4 倍 dim）
    num_heads = 4
    num_experts = 4
    top_k = 2
    batch_size = 2
    seq_len = 10

    # 创建 MoE Transformer 层
    layer = MoETransformerLayer(
        dim=dim,
        num_heads=num_heads,
        num_experts=num_experts,
        top_k=top_k,
        hidden_dim=hidden_dim
    )

    # 随机输入
    x = torch.randn(batch_size, seq_len, dim)

    # 前向传播
    output = layer(x)

    print("✅ 输入形状:", x.shape)      # [2, 10, 256]
    print("✅ 输出形状:", output.shape)  # [2, 10, 256]
    print("✅ MoE 层已成功集成到 Transformer 中！")