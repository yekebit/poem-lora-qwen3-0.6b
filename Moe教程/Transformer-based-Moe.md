å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æˆ‘å°†ä¸ºä½ æä¾›ä¸€ä¸ª**è´´è¿‘ç°ä»£å¤§æ¨¡å‹ï¼ˆå¦‚ Mixtralã€Qwen-MoEï¼‰ç»“æ„çš„ MoE å®ç°**ï¼Œå¹¶åµŒå…¥åˆ° **æ ‡å‡† Transformer å±‚ä¸­**ã€‚ä»£ç ä¼šï¼š

- ä½¿ç”¨ **çœŸæ­£çš„ FFNï¼ˆå‰é¦ˆç½‘ç»œï¼‰ä½œä¸ºä¸“å®¶**ï¼ˆè€Œéå•ä¸ª Linearï¼‰
- æ”¯æŒ **ç¨€ç– top-k æ¿€æ´»**
- å®Œå…¨å…¼å®¹ **PyTorch + Hugging Face é£æ ¼**
- åŒ…å«**é€è¡Œè¯¦ç»†æ³¨é‡Š**ï¼Œé€‚åˆ PyTorch åˆå­¦è€…
- å¯ç›´æ¥ç”¨äºè®­ç»ƒæˆ–æ¨ç†

---

## âœ… æœ€ç»ˆç›®æ ‡ç»“æ„ï¼ˆç°ä»£ MoE Transformer å±‚ï¼‰

```
Input
  â”‚
  â”œâ”€â†’ LayerNorm â†’ Self-Attention â†’ Add & Norm
  â”‚
  â””â”€â†’ LayerNorm â†’ **MoE (FFN Experts)** â†’ Add & Norm â†’ Output
```

> ğŸ”‘ **å…³é”®**ï¼šåªæœ‰ **FFN éƒ¨åˆ†è¢«æ›¿æ¢ä¸º MoE**ï¼ŒAttention ä»æ˜¯å…±äº«çš„ã€‚

---

## ğŸ“¦ å®Œæ•´ä»£ç ï¼ˆå¸¦è¶…è¯¦ç»†æ³¨é‡Šï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


# ================================
# 1. æ ‡å‡† FFNï¼ˆå‰é¦ˆç½‘ç»œï¼‰â€”â€” è¿™å°±æ˜¯â€œä¸“å®¶â€çš„çœŸå®å½¢æ€
# ================================
class FeedForward(nn.Module):
    """
    ä¸€ä¸ªæ ‡å‡†çš„ Transformer å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼š
      x â†’ Linear â†’ GELU/SiLU â†’ Dropout â†’ Linear
    åœ¨ Llama/Qwen ä¸­ï¼Œä¸­é—´å±‚ç»´åº¦é€šå¸¸æ˜¯è¾“å…¥çš„ 4 å€ï¼ˆå¦‚ 4096 â†’ 11008ï¼‰
    """
    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim)      # ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆå‡ç»´ï¼‰
        self.w2 = nn.Linear(hidden_dim, dim)      # ç¬¬äºŒä¸ªçº¿æ€§å±‚ï¼ˆé™ç»´ï¼‰
        self.dropout = nn.Dropout(dropout)
        # æ³¨æ„ï¼šLlama ç”¨ SiLUï¼ŒGPT ç”¨ GELUï¼Œè¿™é‡Œç”¨ GELUï¼ˆæ›´é€šç”¨ï¼‰
        self.act = nn.GELU()

    def forward(self, x):
        # x: [batch, seq_len, dim]
        x = self.w1(x)          # [B, L, hidden_dim]
        x = self.act(x)         # æ¿€æ´»å‡½æ•°
        x = self.dropout(x)
        x = self.w2(x)          # [B, L, dim]
        return x


# ================================
# 2. ç¨€ç– MoE å±‚ â€”â€” ç”¨å¤šä¸ª FFN ä½œä¸ºä¸“å®¶
# ================================
class SparseMoELayer(nn.Module):
    """
    ç¨€ç– Mixture of Experts å±‚ï¼š
      - é—¨æ§ç½‘ç»œé€‰æ‹© top-k ä¸“å®¶
      - æ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ªå®Œæ•´çš„ FFN
      - åªæœ‰è¢«é€‰ä¸­çš„ä¸“å®¶å‚ä¸è®¡ç®—ï¼ˆæ¢¯åº¦ç¨€ç–ï¼‰
    """
    def __init__(
        self,
        num_experts: int,
        top_k: int,
        dim: int,               # è¾“å…¥/è¾“å‡ºç»´åº¦ï¼ˆå¦‚ 4096ï¼‰
        hidden_dim: int,        # FFN ä¸­é—´å±‚ç»´åº¦ï¼ˆå¦‚ 11008ï¼‰
        dropout: float = 0.1
    ):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.dim = dim

        # åˆ›å»º num_experts ä¸ªç‹¬ç«‹çš„ FFN ä¸“å®¶
        self.experts = nn.ModuleList([
            FeedForward(dim, hidden_dim, dropout) for _ in range(num_experts)
        ])

        # é—¨æ§ç½‘ç»œï¼šè¾“å…¥ â†’ ä¸“å®¶ logits
        # æ³¨æ„ï¼šè¿™é‡Œåªç”¨ä¸€ä¸ª Linearï¼Œä¸åŠ æ¿€æ´»å‡½æ•°
        self.gate = nn.Linear(dim, num_experts, bias=False)

    def forward(self, x):
        """
        è¾“å…¥: x.shape = [batch_size, seq_len, dim]
        è¾“å‡º: y.shape = [batch_size, seq_len, dim]
        """
        batch_size, seq_len, dim = x.shape
        assert dim == self.dim, f"è¾“å…¥ç»´åº¦ {dim} ä¸åŒ¹é… MoE å±‚ç»´åº¦ {self.dim}"

        # Step 1: å±•å¹³ token ç»´åº¦ï¼Œä¾¿äºå¤„ç†
        # [B, L, D] â†’ [B*L, D]
        x_flat = x.view(-1, dim)  # total_tokens = B * L

        # Step 2: é—¨æ§æ‰“åˆ†
        gate_logits = self.gate(x_flat)  # [B*L, E]

        # Step 3: é€‰æ‹© top-k ä¸“å®¶
        # topk_vals: top-k çš„ logits å€¼       â†’ [B*L, k]
        # topk_idxs: top-k çš„ä¸“å®¶ç´¢å¼•        â†’ [B*L, k]
        topk_vals, topk_idxs = torch.topk(gate_logits, self.top_k, dim=-1)
        topk_weights = F.softmax(topk_vals, dim=-1)  # [B*L, k]ï¼Œå½’ä¸€åŒ–

        # Step 4: åˆå§‹åŒ–è¾“å‡ºï¼ˆå…¨é›¶ï¼‰
        output_flat = torch.zeros_like(x_flat)  # [B*L, D]

        # Step 5: ã€æ ¸å¿ƒã€‘éå†æ¯ä¸ªä¸“å®¶ï¼Œç´¯åŠ å…¶è´¡çŒ®
        for i, expert in enumerate(self.experts):
            # æ‰¾å‡ºå“ªäº› token é€‰æ‹©äº†å½“å‰ä¸“å®¶ i
            selected = (topk_idxs == i)  # [B*L, k], bool

            if not selected.any():
                continue  # å¦‚æœæ²¡äººé€‰è¿™ä¸ªä¸“å®¶ï¼Œè·³è¿‡

            # æå–æ¯ä¸ª token åˆ†é…ç»™ä¸“å®¶ i çš„æ€»æƒé‡
            # å°†æœªé€‰ä¸­çš„ä½ç½®ç½® 0ï¼Œç„¶åå¯¹ k ä¸ªä½ç½®æ±‚å’Œ
            weights = torch.where(selected, topk_weights, 0.0).sum(dim=-1)  # [B*L]

            # è®¡ç®—å½“å‰ä¸“å®¶å¯¹æ‰€æœ‰ token çš„è¾“å‡º
            expert_out = expert(x_flat)  # [B*L, D]

            # åŠ æƒç´¯åŠ ï¼šweights [B*L, 1] * expert_out [B*L, D] â†’ [B*L, D]
            output_flat += weights.unsqueeze(-1) * expert_out

        # Step 6: æ¢å¤åŸå§‹å½¢çŠ¶
        output = output_flat.view(batch_size, seq_len, dim)  # [B, L, D]
        return output


# ================================
# 3. å®Œæ•´çš„ MoE Transformer å±‚
# ================================
class MoETransformerLayer(nn.Module):
    """
    ä¸€ä¸ªå®Œæ•´çš„ Transformer å±‚ï¼Œå…¶ä¸­ FFN è¢«æ›¿æ¢ä¸º MoEã€‚
    ç»“æ„ï¼š
      x â†’ LayerNorm â†’ Self-Attention â†’ Residual â†’ LayerNorm â†’ MoE â†’ Residual
    """
    def __init__(
        self,
        dim: int,
        num_heads: int,
        num_experts: int,
        top_k: int,
        hidden_dim: int,
        dropout: float = 0.1
    ):
        super().__init__()
        self.dim = dim

        # ========== Self-Attention éƒ¨åˆ†ï¼ˆæ ‡å‡†å®ç°ï¼‰==========
        self.attn_norm = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(
            embed_dim=dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True  # è¾“å…¥ shape: [B, L, D]
        )

        # ========== MoE FFN éƒ¨åˆ† ==========
        self.moe_norm = nn.LayerNorm(dim)
        self.moe = SparseMoELayer(
            num_experts=num_experts,
            top_k=top_k,
            dim=dim,
            hidden_dim=hidden_dim,
            dropout=dropout
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, attn_mask=None):
        """
        x: [batch_size, seq_len, dim]
        attn_mask: å¯é€‰çš„ attention mask
        """
        # --- Self-Attention ---
        residual = x
        x = self.attn_norm(x)
        attn_output, _ = self.attn(x, x, x, attn_mask=attn_mask)
        x = residual + self.dropout(attn_output)

        # --- MoE FFN ---
        residual = x
        x = self.moe_norm(x)
        moe_output = self.moe(x)
        x = residual + self.dropout(moe_output)

        return x


# ================================
# 4. ä½¿ç”¨ç¤ºä¾‹
# ================================
if __name__ == "__main__":
    # è¶…å‚æ•°ï¼ˆæ¨¡æ‹Ÿ Llama é£æ ¼ï¼‰
    dim = 256          # æ¨¡å‹ç»´åº¦ï¼ˆå°è§„æ¨¡æ¼”ç¤ºï¼‰
    hidden_dim = 512   # FFN ä¸­é—´å±‚ï¼ˆé€šå¸¸ 2~4 å€ dimï¼‰
    num_heads = 4
    num_experts = 4
    top_k = 2
    batch_size = 2
    seq_len = 10

    # åˆ›å»º MoE Transformer å±‚
    layer = MoETransformerLayer(
        dim=dim,
        num_heads=num_heads,
        num_experts=num_experts,
        top_k=top_k,
        hidden_dim=hidden_dim
    )

    # éšæœºè¾“å…¥
    x = torch.randn(batch_size, seq_len, dim)

    # å‰å‘ä¼ æ’­
    output = layer(x)

    print("âœ… è¾“å…¥å½¢çŠ¶:", x.shape)      # [2, 10, 256]
    print("âœ… è¾“å‡ºå½¢çŠ¶:", output.shape)  # [2, 10, 256]
    print("âœ… MoE å±‚å·²æˆåŠŸé›†æˆåˆ° Transformer ä¸­ï¼")
```

---

## ğŸ” å…³é”®è®¾è®¡è¯´æ˜ï¼ˆé’ˆå¯¹åˆå­¦è€…ï¼‰

### 1. **ä¸ºä»€ä¹ˆä¸“å®¶æ˜¯ `FeedForward` è€Œä¸æ˜¯ `Linear`ï¼Ÿ**
- çœŸå®å¤§æ¨¡å‹ï¼ˆLlama/Mixtralï¼‰ä¸­ï¼Œ**æ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ªå®Œæ•´çš„ FFN**ï¼ˆä¸¤å±‚ Linear + æ¿€æ´»å‡½æ•°ï¼‰
- å•ä¸ª Linear è¡¨è¾¾èƒ½åŠ›å¤ªå¼±ï¼Œæ— æ³•å­˜å‚¨å¤æ‚çŸ¥è¯†

### 2. **ä¸ºä»€ä¹ˆåªæ›¿æ¢ FFNï¼Œä¸æ›¿æ¢ Attentionï¼Ÿ**
- Attention è´Ÿè´£ **token é—´å…³ç³»å»ºæ¨¡**ï¼Œé€‚åˆå…±äº«
- FFN è´Ÿè´£ **çŸ¥è¯†å­˜å‚¨ä¸éçº¿æ€§å˜æ¢**ï¼Œé€‚åˆä¸“ä¸šåŒ–åˆ†å·¥

### 3. **`top_k=2` æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ**
- æ¯ä¸ª token åªæ¿€æ´» 2 ä¸ªä¸“å®¶ï¼ˆå¦‚ Mixtralï¼‰
- æ€»å‚æ•°é‡ = 4 ä¸ªä¸“å®¶ Ã— FFN å‚æ•°ï¼Œä½†æ¯ token åªç”¨ 2 ä¸ª

### 4. **å¦‚ä½•æ‰©å±•åˆ°æ›´å¤§æ¨¡å‹ï¼Ÿ**
- å¢å¤§ `dim`ï¼ˆå¦‚ 4096ï¼‰ã€`hidden_dim`ï¼ˆå¦‚ 11008ï¼‰
- å¢åŠ  `num_experts`ï¼ˆå¦‚ 8ã€16ï¼‰
- å †å å¤šä¸ª `MoETransformerLayer`

---

## ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®

1. **è®­ç»ƒå®ƒ**ï¼šåŒ…è£…æˆå®Œæ•´æ¨¡å‹ï¼ŒåŠ  Embedding + LM Head
2. **åŠ è´Ÿè½½å‡è¡¡æŸå¤±**ï¼šé˜²æ­¢ä¸“å®¶åå¡Œ
3. **æ›¿æ¢ä¸º Qwen/Llama é£æ ¼çš„ SiLU æ¿€æ´»**
4. **ç”¨ vLLM éƒ¨ç½²**ï¼šæ”¯æŒé«˜æ•ˆæ¨ç†

---

