import os
import torch
import matplotlib.pyplot as plt
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model
from tqdm import tqdm

# =========================
# 0. è·¯å¾„è®¾ç½®
# =========================
MODEL_PATH = "/home/dsl/learn/poem/qwen3-0_6b"
TRAIN_FILE = "/home/dsl/learn/poem/processed_data/data_new/train.jsonl"
VAL_FILE = "/home/dsl/learn/poem/processed_data/data_new/val.jsonl"
OUTPUT_DIR = "/home/dsl/learn/poem/output/qwen3-poem-lora-new"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# =========================
# 1. åŠ è½½ tokenizer
# =========================
print("ğŸ”§ åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# =========================
# 2. æ„å»º prompt
# =========================
def build_prompt(example):
    """
    æ„å»ºè®­ç»ƒ prompt
    """
    return f"### Instruction:\n{example['instruction']}\n\n### Response:\n{example['output']}"

# =========================
# 3. æ•°æ®é›†åŠ è½½ & tokenize
# =========================
print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
dataset = load_dataset("json", data_files={"train": TRAIN_FILE, "validation": VAL_FILE})

def tokenize_fn(example):
    """
    å¯¹å•æ¡æ ·æœ¬è¿›è¡Œ tokenizationï¼Œå¹¶åœ¨ output æœ«å°¾åŠ  EOS
    instruction éƒ¨åˆ†ä¸è®¡ç®— loss
    """
    prompt_text = build_prompt(example)
    # ç¼–ç 
    tokens = tokenizer(prompt_text, truncation=True, max_length=512, padding="max_length")

    # è®¡ç®— instruction é•¿åº¦
    instruction_len = len(tokenizer(f"### Instruction:\n{example['instruction']}\n\n### Response:\n")["input_ids"])
    
    # æ„å»º labels
    labels = tokens["input_ids"].copy()
    # instruction éƒ¨åˆ† loss ä¸è®¡ç®—
    labels[:instruction_len] =  [-100] * instruction_len

    # output æœ«å°¾åŠ  EOSï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
    if labels[-1] != tokenizer.eos_token_id:
        labels.append(tokenizer.eos_token_id)
        tokens["input_ids"].append(tokenizer.eos_token_id)
        tokens["attention_mask"].append(1)

    tokens["labels"] = labels
    return tokens

print("ğŸ”„ Tokenizing æ•°æ®é›†...")
tokenized_dataset = dataset.map(
    tokenize_fn,
    remove_columns=dataset["train"].column_names,
    batched=False,
    desc="Tokenizing"
)

print(f"è®­ç»ƒé›†å¤§å°: {len(tokenized_dataset['train'])}, éªŒè¯é›†å¤§å°: {len(tokenized_dataset['validation'])}")

# =========================
# 4. åŠ è½½åŸºç¡€æ¨¡å‹
# =========================
print("ğŸ§  åŠ è½½åŸºç¡€æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# =========================
# 5. LoRA é…ç½®
# =========================
print("ğŸ§© é…ç½® LoRA å¾®è°ƒ...")
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    target_modules=["q_proj","k_proj","v_proj","o_proj"],
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# =========================
# 6. è®­ç»ƒå‚æ•°
# =========================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    num_train_epochs=3,
    logging_steps=50,
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    bf16=True,
    report_to="none",
    load_best_model_at_end=True
)

# =========================
# 7. æ•°æ®æ•´ç†å™¨
# =========================
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# =========================
# 8. Trainer
# =========================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    data_collator=data_collator
)

# =========================
# 9. è®­ç»ƒ
# =========================
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
train_history = trainer.train()
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# =========================
# 10. Loss å¯è§†åŒ–
# =========================
log_history = trainer.state.log_history

# è®­ç»ƒ loss
train_logs = [log for log in log_history if "loss" in log and "eval_loss" not in log]
train_steps = [log["step"] for log in train_logs]
train_losses = [log["loss"] for log in train_logs]

# éªŒè¯ loss
eval_logs = [log for log in log_history if "eval_loss" in log]
eval_steps = [log["step"] for log in eval_logs]
eval_losses = [log["eval_loss"] for log in eval_logs]

# ç»˜åˆ¶
plt.figure(figsize=(10, 6))
if train_losses:
    plt.plot(train_steps, train_losses, label="Train Loss", marker='o', linestyle='-')
if eval_losses:
    plt.plot(eval_steps, eval_losses, label="Eval Loss", marker='s', linestyle='--')
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.title("LoRA Fine-tuning Loss Curve")
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig(os.path.join(OUTPUT_DIR, "loss_curve.png"), dpi=150, bbox_inches='tight')
plt.show()

print(f"âœ… Loss æ›²çº¿å·²ä¿å­˜åˆ° {OUTPUT_DIR}/loss_curve.png")