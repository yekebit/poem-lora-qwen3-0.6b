å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æˆ‘å°†ä¸ºä½ æä¾›ä¸€ä»½**ä¸¥æ ¼éµå¾ª 2017 å¹´ã€ŠAttention Is All You Needã€‹åŸå§‹è®ºæ–‡**çš„ **â€œæ‰‹æ’• Transformerâ€** å®ç°ã€‚ä»£ç ï¼š

- âœ… **å®Œå…¨å¤ç°è®ºæ–‡ç»“æ„**ï¼ˆEncoder-Decoder æ¶æ„ï¼‰
- âœ… **é€è¡Œè¯¦ç»†æ³¨é‡Š**ï¼Œé€‚åˆ PyTorch åˆå­¦è€…
- âœ… **åŒ…å«æ‰€æœ‰æ ¸å¿ƒç»„ä»¶**ï¼šMulti-Head Attentionã€Positional Encodingã€LayerNormã€æ®‹å·®è¿æ¥
- âœ… **å¯ç›´æ¥è¿è¡Œ**ï¼Œé™„å¸¦ç®€å•è®­ç»ƒç¤ºä¾‹
- âœ… **ä¸ä¾èµ– Hugging Face**ï¼Œçº¯ PyTorch å®ç°

---

## ğŸ“˜ ä¸€ã€Transformer åŸå§‹æ¶æ„å›é¡¾ï¼ˆè®ºæ–‡ Figure 1ï¼‰

```
Encoder: [Input Embedding + PE] â†’ Nx [MultiHeadAttn â†’ FFN]
Decoder: [Output Embedding + PE] â†’ Nx [MaskedMultiHeadAttn â†’ MultiHeadAttn â†’ FFN]
```

> ğŸ”‘ **å…³é”®ç»„ä»¶**ï¼š
> - **Scaled Dot-Product Attention**
> - **Multi-Head Attention**
> - **Positional Encoding**
> - **Layer Normalization + Residual Connection**
> - **Feed-Forward Network**

---

## ğŸ“¦ äºŒã€å®Œæ•´ä»£ç å®ç°ï¼ˆå¸¦è¶…è¯¦ç»†æ³¨é‡Šï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np


# ================================
# 1. Scaled Dot-Product Attentionï¼ˆè®ºæ–‡ Section 3.2.1ï¼‰
# ================================
def scaled_dot_product_attention(q, k, v, mask=None):
    """
    è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
    Args:
        q: [batch_size, heads, seq_len_q, d_k]
        k: [batch_size, heads, seq_len_k, d_k]
        v: [batch_size, heads, seq_len_v, d_v] (seq_len_k == seq_len_v)
        mask: [batch_size, 1, 1, seq_len_k] æˆ– [batch_size, 1, seq_len_q, seq_len_k]
    Returns:
        output: [batch_size, heads, seq_len_q, d_v]
        attention_weights: [batch_size, heads, seq_len_q, seq_len_k]
    """
    # Step 1: è®¡ç®— QK^T / sqrt(d_k)
    d_k = q.size(-1)
    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # [B, H, Lq, Lk]

    # Step 2: åº”ç”¨ maskï¼ˆå¯é€‰ï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # Step 3: Softmax è·å–æ³¨æ„åŠ›æƒé‡
    attention_weights = F.softmax(scores, dim=-1)  # [B, H, Lq, Lk]

    # Step 4: åŠ æƒæ±‚å’Œ V
    output = torch.matmul(attention_weights, v)  # [B, H, Lq, Dv]
    return output, attention_weights


# ================================
# 2. Multi-Head Attentionï¼ˆè®ºæ–‡ Section 3.2.2ï¼‰
# ================================
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        """
        d_model: æ¨¡å‹ç»´åº¦ï¼ˆå¦‚ 512ï¼‰
        num_heads: æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¦‚ 8ï¼‰
        """
        super().__init__()
        assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦
        self.d_v = self.d_k

        # å®šä¹‰çº¿æ€§å˜æ¢å±‚ï¼ˆè®ºæ–‡ä¸­çš„ W^Q, W^K, W^V, W^Oï¼‰
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)

    def split_heads(self, x, batch_size):
        """
        å°†è¾“å…¥åˆ†å‰²ä¸ºå¤šå¤´
        è¾“å…¥: [batch_size, seq_len, d_model]
        è¾“å‡º: [batch_size, num_heads, seq_len, d_k]
        """
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)  # [B, H, L, D]

    def forward(self, q, k, v, mask=None):
        """
        q, k, v: [batch_size, seq_len, d_model]
        mask: [batch_size, 1, seq_len] æˆ– [batch_size, seq_len, seq_len]
        """
        batch_size = q.size(0)

        # Step 1: çº¿æ€§å˜æ¢
        q = self.w_q(q)  # [B, Lq, D]
        k = self.w_k(k)  # [B, Lk, D]
        v = self.w_v(v)  # [B, Lv, D]

        # Step 2: åˆ†å‰²ä¸ºå¤šå¤´
        q = self.split_heads(q, batch_size)  # [B, H, Lq, Dk]
        k = self.split_heads(k, batch_size)  # [B, H, Lk, Dk]
        v = self.split_heads(v, batch_size)  # [B, H, Lv, Dv]

        # Step 3: è®¡ç®—æ³¨æ„åŠ›
        attention_output, attention_weights = scaled_dot_product_attention(q, k, v, mask)

        # Step 4: åˆå¹¶å¤šå¤´
        attention_output = attention_output.transpose(1, 2).contiguous()  # [B, Lq, H, Dv]
        attention_output = attention_output.view(batch_size, -1, self.d_model)  # [B, Lq, D]

        # Step 5: æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.w_o(attention_output)  # [B, Lq, D]
        return output, attention_weights


# ================================
# 3. Position-wise Feed-Forward Networkï¼ˆè®ºæ–‡ Section 3.3ï¼‰
# ================================
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        d_model: è¾“å…¥/è¾“å‡ºç»´åº¦
        d_ff: éšè—å±‚ç»´åº¦ï¼ˆè®ºæ–‡ä¸­ä¸º 2048ï¼Œd_model=512ï¼‰
        """
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ReLU()  # è®ºæ–‡ä½¿ç”¨ ReLU

    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        """
        x = self.linear1(x)          # [B, L, d_ff]
        x = self.activation(x)       # ReLU
        x = self.dropout(x)
        x = self.linear2(x)          # [B, L, d_model]
        return x


# ================================
# 4. Positional Encodingï¼ˆè®ºæ–‡ Section 3.5ï¼‰
# ================================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        """
        d_model: æ¨¡å‹ç»´åº¦
        max_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        super().__init__()
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ [max_len, d_model]
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        
        # è®¡ç®— div_term = exp(-ln(10000) * 2i / d_model)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        )  # [d_model/2]
        
        # å¶æ•°ç»´ç”¨ sinï¼Œå¥‡æ•°ç»´ç”¨ cos
        pe[:, 0::2] = torch.sin(position * div_term)  # [max_len, d_model/2]
        pe[:, 1::2] = torch.cos(position * div_term)  # [max_len, d_model/2]
        
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)  # ä¸ä½œä¸ºå‚æ•°ï¼Œä½†ä¿å­˜åœ¨æ¨¡å‹ä¸­

    def forward(self, x):
        """
        x: [batch_size, seq_len, d_model]
        """
        x = x + self.pe[:, :x.size(1), :]  # å¹¿æ’­ç›¸åŠ 
        return x


# ================================
# 5. Encoder Layerï¼ˆè®ºæ–‡ Figure 1 å·¦ä¾§ï¼‰
# ================================
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        # Layer Normalizationï¼ˆè®ºæ–‡ Section 3.1ï¼‰
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        x: [batch_size, seq_len, d_model]
        mask: [batch_size, 1, seq_len]ï¼ˆç”¨äº padding maskï¼‰
        """
        # Sub-layer 1: Multi-Head Self-Attention + Residual + Norm
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Sub-layer 2: Feed-Forward + Residual + Norm
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x


# ================================
# 6. Decoder Layerï¼ˆè®ºæ–‡ Figure 1 å³ä¾§ï¼‰
# ================================
class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        # Masked Multi-Head Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼Œå¸¦ maskï¼‰
        self.masked_attn = MultiHeadAttention(d_model, num_heads)
        # Multi-Head Attentionï¼ˆç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼‰
        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):
        """
        x: [batch_size, tgt_len, d_model]ï¼ˆè§£ç å™¨è¾“å…¥ï¼‰
        enc_output: [batch_size, src_len, d_model]ï¼ˆç¼–ç å™¨è¾“å‡ºï¼‰
        src_mask: [batch_size, 1, src_len]ï¼ˆæºåºåˆ— padding maskï¼‰
        tgt_mask: [batch_size, tgt_len, tgt_len]ï¼ˆç›®æ ‡åºåˆ— maskï¼Œå« look-ahead maskï¼‰
        """
        # Sub-layer 1: Masked Multi-Head Self-Attention
        masked_attn_output, _ = self.masked_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(masked_attn_output))
        
        # Sub-layer 2: Multi-Head Attention over Encoder Output
        enc_dec_attn_output, _ = self.enc_dec_attn(x, enc_output, enc_output, src_mask)
        x = self.norm2(x + self.dropout(enc_dec_attn_output))
        
        # Sub-layer 3: Feed-Forward
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x


# ================================
# 7. å®Œæ•´ Transformerï¼ˆè®ºæ–‡ Figure 1ï¼‰
# ================================
class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        tgt_vocab_size,
        d_model=512,
        num_heads=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        d_ff=2048,
        max_seq_len=5000,
        dropout=0.1
    ):
        super().__init__()
        self.d_model = d_model
        
        # Embedding å±‚ï¼ˆè®ºæ–‡ Section 3.4ï¼‰
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # Positional Encoding
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # Encoder Stack
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_encoder_layers)
        ])
        
        # Decoder Stack
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_decoder_layers)
        ])
        
        # Final Linear + Softmaxï¼ˆè®ºæ–‡ Section 3.4ï¼‰
        self.final_linear = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)

    def make_src_mask(self, src):
        """
        åˆ›å»ºæºåºåˆ— maskï¼ˆå¿½ç•¥ paddingï¼‰
        src: [batch_size, src_len]
        è¿”å›: [batch_size, 1, src_len]
        """
        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # [B, 1, 1, L]
        return src_mask

    def make_tgt_mask(self, tgt):
        """
        åˆ›å»ºç›®æ ‡åºåˆ— maskï¼ˆpadding + look-aheadï¼‰
        tgt: [batch_size, tgt_len]
        è¿”å›: [batch_size, 1, tgt_len, tgt_len]
        """
        batch_size, tgt_len = tgt.shape
        
        # Padding mask
        pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)  # [B, 1, 1, L]
        
        # Look-ahead maskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
        subsequent_mask = torch.tril(torch.ones((tgt_len, tgt_len))).bool()
        subsequent_mask = subsequent_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, L, L]
        
        # åˆå¹¶ä¸¤ç§ mask
        tgt_mask = pad_mask & subsequent_mask.to(tgt.device)
        return tgt_mask

    def forward(self, src, tgt):
        """
        src: [batch_size, src_len]
        tgt: [batch_size, tgt_len]
        """
        # Step 1: åˆ›å»º masks
        src_mask = self.make_src_mask(src)
        tgt_mask = self.make_tgt_mask(tgt)
        
        # Step 2: ç¼–ç å™¨
        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)  # [B, L, D]
        src_emb = self.pos_encoding(src_emb)
        src_emb = self.dropout(src_emb)
        
        enc_output = src_emb
        for layer in self.encoder_layers:
            enc_output = layer(enc_output, src_mask)
        
        # Step 3: è§£ç å™¨
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)  # [B, L, D]
        tgt_emb = self.pos_encoding(tgt_emb)
        tgt_emb = self.dropout(tgt_emb)
        
        dec_output = tgt_emb
        for layer in self.decoder_layers:
            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)
        
        # Step 4: æœ€ç»ˆè¾“å‡º
        output = self.final_linear(dec_output)  # [B, L, vocab_size]
        return output


# ================================
# 8. ä½¿ç”¨ç¤ºä¾‹
# ================================
if __name__ == "__main__":
    # è¶…å‚æ•°ï¼ˆå°è§„æ¨¡æ¼”ç¤ºï¼‰
    src_vocab_size = 1000
    tgt_vocab_size = 1000
    d_model = 128      # è®ºæ–‡ç”¨ 512ï¼Œè¿™é‡Œå‡å°ä¾¿äºæ¼”ç¤º
    num_heads = 4      # å¿…é¡»æ•´é™¤ d_model
    d_ff = 256         # è®ºæ–‡ç”¨ 2048
    
    # åˆ›å»ºæ¨¡å‹
    model = Transformer(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=d_model,
        num_heads=num_heads,
        d_ff=d_ff,
        num_encoder_layers=2,   # è®ºæ–‡ç”¨ 6ï¼Œè¿™é‡Œç”¨ 2
        num_decoder_layers=2
    )
    
    # éšæœºè¾“å…¥ï¼ˆ0 è¡¨ç¤º paddingï¼‰
    batch_size = 4
    src_len = 10
    tgt_len = 8
    
    src = torch.randint(1, src_vocab_size, (batch_size, src_len))  # [4, 10]
    tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_len))  # [4, 8]
    
    # å‰å‘ä¼ æ’­
    output = model(src, tgt)
    
    print("âœ… æºè¾“å…¥å½¢çŠ¶:", src.shape)        # [4, 10]
    print("âœ… ç›®æ ‡è¾“å…¥å½¢çŠ¶:", tgt.shape)      # [4, 8]
    print("âœ… è¾“å‡ºå½¢çŠ¶:", output.shape)       # [4, 8, 1000]
    print("âœ… Transformer å®ç°æˆåŠŸï¼")
```

---

## ğŸ” ä¸‰ã€å…³é”®å®ç°ç»†èŠ‚è¯´æ˜

### 1. **ä¸ºä»€ä¹ˆ Embedding è¦ä¹˜ä»¥ `sqrt(d_model)`ï¼Ÿ**
- è®ºæ–‡ Section 3.4 æåˆ°ï¼š**"We also multiply the embeddings by sqrt(d_model)"**
- ç›®çš„ï¼šä½¿ Embedding ä¸ Positional Encoding çš„ scale ç›¸å½“

### 2. **Mask çš„ä¸¤ç§ç±»å‹**
- **Padding Mask**ï¼šå¿½ç•¥åºåˆ—ä¸­çš„ padding tokenï¼ˆå€¼ä¸º 0ï¼‰
- **Look-ahead Mask**ï¼šè§£ç å™¨ä¸­é˜²æ­¢çœ‹åˆ°æœªæ¥ tokenï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰

### 3. **Layer Normalization ä½ç½®**
- è®ºæ–‡ä½¿ç”¨ **Post-LN**ï¼ˆå…ˆæ®‹å·®å† LayerNormï¼‰
- åç»­ç ”ç©¶ï¼ˆå¦‚ Pre-LNï¼‰æ•ˆæœæ›´å¥½ï¼Œä½†è¿™é‡Œä¸¥æ ¼éµå¾ªåŸè®ºæ–‡

### 4. **Positional Encoding å…¬å¼**
- $ PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{\text{model}}}) $
- $ PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}}) $

---

## ğŸš€ å››ã€å¦‚ä½•æ‰©å±•ï¼Ÿ

1. **è®­ç»ƒçœŸå®ç¿»è¯‘ä»»åŠ¡**ï¼šæ›¿æ¢ä¸º WMT æ•°æ®é›†
2. **å¢åŠ  Dropout**ï¼šåœ¨ Embedding å’Œ Attention ååŠ  Dropout
3. **Label Smoothing**ï¼šæ”¹è¿›æŸå¤±å‡½æ•°
4. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šä½¿ç”¨è®ºæ–‡ä¸­çš„ warmup ç­–ç•¥

