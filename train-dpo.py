import os
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
from trl import DPOTrainer, DPOConfig  # ğŸ‘ˆ æ–°å¢ DPO ç»„ä»¶

# =========================
# 0. è·¯å¾„è®¾ç½®
# =========================
MODEL_PATH = "/home/dsl/learn/poem/qwen3-0_6b"
TRAIN_FILE = "/home/dsl/learn/poem/processed_data/data_dpo/train.jsonl"  # ğŸ‘ˆ DPO åå¥½æ•°æ®
OUTPUT_DIR = "/home/dsl/learn/poem/output/qwen3-poem-dpo"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# =========================
# 1. åŠ è½½ tokenizer
# =========================
print("ğŸ”§ åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# =========================
# 2. æ„å»º promptï¼ˆä»…ç”¨äº DPO è¾“å…¥ï¼‰
# =========================
def build_prompt(instruction):
    return f"### Instruction:\n{instruction}\n\n### Response:\n"

# =========================
# 3. åŠ è½½ DPO åå¥½æ•°æ®é›†
# =========================
print("ğŸ“‚ åŠ è½½ DPO åå¥½æ•°æ®é›†...")
dataset = load_dataset("json", data_files={"train": TRAIN_FILE})

# æ„é€  DPO æ‰€éœ€çš„ä¸‰åˆ—: prompt, chosen, rejected
def preprocess(example):
    return {
        "prompt": build_prompt(example["instruction"]),
        "chosen": example["chosen"],
        "rejected": example["rejected"]
    }

dpo_dataset = dataset["train"].map(
    preprocess,
    remove_columns=dataset["train"].column_names,  # ç§»é™¤åŸå§‹åˆ—
    desc="æ„å»º DPO æ ¼å¼"
)

print(f"âœ… DPO æ•°æ®é›†å¤§å°: {len(dpo_dataset)}")

# =========================
# 4. åŠ è½½åŸºç¡€æ¨¡å‹
# =========================
print("ğŸ§  åŠ è½½åŸºç¡€æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# =========================
# 5. LoRA é…ç½®ï¼ˆä¸ SFT ç›¸åŒï¼‰
# =========================
print("ğŸ§© é…ç½® LoRA å¾®è°ƒ...")
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["q_proj","k_proj","v_proj","o_proj"],
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# =========================
# 6. DPO è®­ç»ƒå‚æ•°ï¼ˆå…³é”®è°ƒæ•´ï¼ï¼‰
# =========================
training_args = DPOConfig(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=2,          # ğŸ‘ˆ DPO æ˜¾å­˜é«˜ï¼Œbatch_size å‡å°
    gradient_accumulation_steps=8,          # ç­‰æ•ˆ batch_size=16
    learning_rate=1e-5,                    # ğŸ‘ˆ DPO é€šå¸¸ç”¨æ›´å° lr
    num_train_epochs=1,                    # ğŸ‘ˆ DPO 1 epoch é€šå¸¸è¶³å¤Ÿ
    beta=0.1,                              # ğŸ‘ˆ DPO æ ¸å¿ƒè¶…å‚ï¼šåå¥½å¼ºåº¦
    logging_steps=10,
    save_strategy="steps",
    save_steps=200,
    bf16=True,
    report_to="none",
        max_length=256,                        # æ€»é•¿åº¦ (prompt + response)
    max_prompt_length=128
)

# =========================
# 7. DPO Trainerï¼ˆæ›¿ä»£åŸ Trainerï¼‰
# =========================
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=None,                        # ğŸ‘ˆ è‡ªåŠ¨ä½¿ç”¨å½“å‰ model ä½œä¸º reference
    args=training_args,
    train_dataset=dpo_dataset,
    processing_class=tokenizer                  # prompt æœ€å¤§é•¿åº¦
)

# =========================
# 8. å¼€å§‹ DPO è®­ç»ƒ
# =========================
print("ğŸš€ å¼€å§‹ DPO è®­ç»ƒ...")
dpo_trainer.train()

# =========================
# 9. ä¿å­˜æ¨¡å‹
# =========================
dpo_trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"âœ… DPO æ¨¡å‹å·²ä¿å­˜åˆ° {OUTPUT_DIR}")